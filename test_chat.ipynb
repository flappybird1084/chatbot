{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158eaa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math, time, os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.amp.autocast_mode import autocast\n",
    "from torch.amp.grad_scaler import GradScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d9467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/software/Documents/.rianstuff/chatbot/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Hello, I feel a bit sad today because things seem hard to understand and move through. [/INST] I understand how you feel; sometimes life can be heavy like a thick substance we cannot lift. [INST] Yes, it can be very difficult, especially for young people trying to find their way. [/INST] Young minds often carry many questions that can weigh them down with worries and doubts. [INST] Sometimes, I wish everything would get better and we could all feel lighter again. [/INST] Hoping for better\n",
      "{'text': \"[INST] Do you think the disease spreading in the city is really as bad as it seems? [/INST] It does seem very clear that many people are crying over the current situation. [INST] Yes, I feel disgusted by how quickly it is spreading without control or care. [/INST] It makes me feel unwell just to think about how people's lives are affected deeply. [INST] I can’t believe some people ignore the danger and spread the disease even more. [/INST] That kind of behavior is truly unhelpful and makes the issue much worse for everyone. [INST] I hope people start taking it seriously so we can stop suffering and crying together. [/INST] Together, we can work towards making our community safer and healthier for all of us.\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\n",
    "dataset = load_dataset(\"starhopp3r/TinyChat\")\n",
    "# This gives you cleaned, plain text articles1\n",
    "print(dataset['train'][100]['text'][:500])  # Print the first 500 characters of the first article\n",
    "print(dataset['train'][600000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd5951c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50257, 18435, 11, 995, 0, 220, 50258]\n",
      "[INST] Hello, world! [/INST]\n"
     ]
    }
   ],
   "source": [
    "base_encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "special_tokens = {\n",
    "    \"[INST]\": base_encoding.n_vocab,       # next available token id\n",
    "    \"[/INST]\": base_encoding.n_vocab + 1\n",
    "}\n",
    "\n",
    "# 3. Create a new encoding that merges GPT‑2’s tokens + your special tokens\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"gpt2_with_inst\",\n",
    "    pat_str=base_encoding._pat_str,\n",
    "    mergeable_ranks=base_encoding._mergeable_ranks,\n",
    "    special_tokens={**base_encoding._special_tokens, **special_tokens},\n",
    ")\n",
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text, allowed_special={\"[INST]\", \"[/INST]\"})\n",
    "\n",
    "def decode(tokens):\n",
    "    return tokenizer.decode(tokens)\n",
    "  \n",
    "print(encode(\"[INST] Hello, world! [/INST]\"))\n",
    "print(decode(encode(\"[INST] Hello, world! [/INST]\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b98c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, block_size):\n",
    "        self.dataset = hf_dataset\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['train'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Start with a random index sample\n",
    "        rand_idx = torch.randint(0, len(self.dataset['train']), (1,)).item()\n",
    "        text = self.dataset['train'][rand_idx]['text']\n",
    "        tokens = encode(text)\n",
    "\n",
    "        # Keep appending more samples if too short\n",
    "        while len(tokens) < self.block_size + 1:\n",
    "            next_idx = torch.randint(0, len(self.dataset['train']), (1,)).item()\n",
    "            next_text = self.dataset['train'][next_idx]['text']\n",
    "            tokens.extend(encode(\" \" + next_text))\n",
    "            # Prevent runaway growth\n",
    "            if len(tokens) > self.block_size * 2:\n",
    "                break\n",
    "\n",
    "        # Truncate to block_size + 1\n",
    "        tokens = torch.tensor(tokens[: self.block_size + 1])\n",
    "\n",
    "        x = tokens[: self.block_size]\n",
    "        y = tokens[1 : self.block_size + 1]\n",
    "        return x.long(), y.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "599aa05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "train_model = False\n",
    "block_size = 128\n",
    "n_layers = 8\n",
    "n_heads = 8\n",
    "dropout_p = 0.1\n",
    "batch_size =8\n",
    "learning_rate = 3e-4\n",
    "n_embedding = 128\n",
    "max_iters = 5000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69561e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "train_dataset = TextDataset(dataset, block_size=block_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5598ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embedding, n_layers, n_heads, dropout_p, block_size):\n",
    "        super(GPTModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embedding)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embedding)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=n_embedding, nhead=n_heads, dropout=dropout_p)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embedding)\n",
    "        self.head = nn.Linear(n_embedding, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz, seq_len = x.size()\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(bsz, seq_len)\n",
    "        x = self.token_embedding(x) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1344ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define objects\n",
    "vocab_size = tokenizer.n_vocab\n",
    "\n",
    "model = GPTModel(vocab_size, n_embedding, n_layers, n_heads, dropout_p, block_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0982489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/software/Documents/.rianstuff/chatbot/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# training loop\n",
    "torch.set_float32_matmul_precision('high')\n",
    "scaler = GradScaler(device)\n",
    "if train_model:\n",
    "    compiled_model = torch.compile(model)\n",
    "\n",
    "    pbar = tqdm(range(max_iters), desc=\"Training\", ncols=100)\n",
    "    data_iter = iter(train_dataloader)\n",
    "\n",
    "    for count in pbar:\n",
    "        # xb, yb = next(data_iter)\n",
    "\n",
    "        try:\n",
    "            xb, yb = next(data_iter)\n",
    "        except StopIteration:\n",
    "            # dataloader exhausted — restart it\n",
    "            data_iter = iter(train_dataloader)\n",
    "            xb, yb = next(data_iter)\n",
    "        if count%100 == 0:\n",
    "          # print out xb, yb, encoded too\n",
    "          print('xb decoded: ', decode(xb[0].tolist())) \n",
    "          print('yb decoded: ', decode(yb[0].tolist())) \n",
    "\n",
    "        # except StopIteration:\n",
    "        #     break  # dataloader exhausted before max_iters\n",
    "        \n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        # logits = compiled_model(xb)\n",
    "        # loss = loss_fn(logits.view(-1, vocab_size), yb.view(-1))\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        with autocast(device, dtype=torch.float16):\n",
    "            logits = compiled_model(xb)\n",
    "            loss = loss_fn(logits.view(-1, vocab_size), yb.view(-1))\n",
    "\n",
    "        # backward pass with gradient scaling\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update bar text dynamically\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb95580",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "  torch.save(model.state_dict(), \"checkpoints/gpt_model-1.pth\")\n",
    "else:\n",
    "  model.load_state_dict(torch.load(\"checkpoints/gpt_model-1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4371725d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 17.677395 million parameters.\n",
      "how are you doing today? [/INST] the the mess really this open today hours someday [/INST] I do It is unpleasant when uncle levels in a balance to share. [INST] Do you feel a complete like blo inside outside, and it feels, like it at [/INST] That would [/INST] keep any little is an away around product. [/INST][INST] I find it feels very due[INST] I agree up [/INST] and not real we making is a so chaotic. [/INST] and hopefully ch, sometimes that we should. [INST] Do[INST] I will believe distribution in our hard it will be heard a better today to take to remind Hello, taking the better today [/INST] Reflect hit up them go for everyone with creativity. [/INST][INST], that[/INST] It about that weighing scared with the happiness en provide. [/INST], reflecting storms can help them onesing. [/INST] Indeed a walk might. [/INST] Definitely make better when reminders will have[/INST] Reflect truly bright go a lovely probably return made. [/INST] St you, each can cause, can help, even through today, and curiosity. [/INST] It surely make out, in the performances. [/INST] I view soon [INST] Hello even persistence Experiment spot experiments and more and comfort can lift myizzy who with sharing connections, or to find happiness. [/INST] al [/INST][/INST] That is the only or in experimenting[INST] I, perhaps. [/INST] hope thatclusive remarkable and comfort and brighter Do steps do, I, we can be lovely and and as every and reflecting. [/INST] I can really were in our. [/INST] When over? [/INST] It heard even. [/INST] begin, we can be, for cooler and help or will work messy and for [INST] I we can look acknowledging and happinesss also go to plan [/INST] I we will definitely; something[INST] Why, that flying a small work and community Did we could ease makes and that is indeed smiling teaches cozy our and clarity about how about that are can explore may and just and healthier [/INST][/INST] without, or should start, as do. [/INST] Change if to turn to share [/INST]. [/INST] Indeed because for awareness and we need start stays to better places can explore [/INST] Definitely hike still to find next [/INST] Many[/INST][INST] How you. [/INST] sailing goes by and peace for special trees can[/INST][/INST] So, I more cheered, even fear that slow can reduce political greatly know or explore feel grateful. [/INST] That often on our together [/INST] Absolutely, we should focus and our, and finds\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens, block_size, device):\n",
    "    model.eval()\n",
    "    # Encode the prompt text into token IDs\n",
    "    tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Only keep the last block_size tokens for context\n",
    "        input_tokens = tokens[:, -block_size:]\n",
    "\n",
    "        # Get logits and take the last token’s distribution\n",
    "        logits = model(input_tokens)\n",
    "        logits = logits[:, -1, :]  # (batch=1, vocab)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample from the distribution\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "    # Decode back into text\n",
    "    output_text = tokenizer.decode(tokens[0].tolist())\n",
    "    return output_text\n",
    "  \n",
    "# print model parameters\n",
    "print (f\"Model has {sum(p.numel() for p in model.parameters())/1000000} million parameters.\")\n",
    "prompt = \"how are you doing today? [/INST]\"\n",
    "print(generate_text(model, tokenizer, prompt, max_new_tokens=500, block_size=block_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9eb22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
